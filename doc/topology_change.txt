Topology Change
===============

A cluster is a set of servers in a data center and serves data from one keyspace.
One server can have multiple keyspaces. Here we only covers topology change in one cluster.

This design covers replacing a server in a keyspace, adding servers to a cluster, removing servers from a cluster.

Replace a server in a keyspace
--------------------------------
Assuming there are a list of nodes with replicated shards.
There are existing shard A and B. Shard C is a new shard that's going to replace A.
B is just one of the peer shards of A.

Notation:
<=> means follow each other
=> means unidirectional follow.
-> means clients read/write to the shard.

0. at the beginning,
    -> A, A <=> B
1. master tell a server to create shard C, and unidirectionally follow A.
1.1 the server creates shard C, and follows A, and follow other peer shards
    -> A, A <=> B, A => C, B => C
1.2 the new shard is marked is_candidate, for normal clients, read/write both switch from A to C.
    (possibly some slow clients) -> A, -> C, A <=> B, A => C, B => C
2. master wait until all "replicate shard prepare" requests are completed, and tell shard Cs to promote themselves, (but not sending shard status to master)
3. master set these shards status to READY, and remove old shard, and add the new shard. Broadcast this to all clients for this keyspace
3.1 all clients remove the old server(clear out the tail offset also), and move the candidate server to the right position.
4. master wait a period of time for the slow clients, tell shard A to cleanup
4.1 shard A wait until no traffic, shuts down
4.2 shard C found A is gone, and not going to follow shard A any more.
4.3 for store clients, shard B as a client detected this change, and starts to follow C from beginning
    -> C, B <=> C


Remove servers from a cluster
--------------------------------
There are existing shard A and B.
B is on one to-be-removed server.
Cluster size is going to change from m to n.

B =>(i/n) A means A pull data on B that matches ith node on the new cluster with size n.

0. at the beginning,
    -> A, -> B
1. master tell all m servers the new cluster size, to create and bootstrap all new or missing shards
1.1. there are new shards to be created in the first and second servers, they should be bootstrapped from the existing shard first.
1.2 shard A follows all main shards on to-be-removed servers.
    -> A, -> B, B =>(i/n) A
2. master tell all servers to commit the changes, promote from CANDIDATE to READY, adjust cluster size, (but not sending shard status to master)
3. master change the cluster size to n. master set these shards status to READY, and remove old shard, and add the new shard. Broadcast this to all clients for this keyspace
3.1 normal clients will read/write by new cluster size n.
3.2 all clients remove the old server(clear out the tail offset also), and move the candidate server to the right position.
3. master tell shard A to commit the cluster size to disk
4. master tell shard B to stop, some servers may have a few shards to be removed.

Add servers to a cluster
--------------------------------
There are existing shard A.
shard B is on one to-be-added server.
Cluster size is going to change from m to n.

B =>(i/n) A means A pull data on B that matches ith node on the new cluster with size n.

0.1 allocate new servers, assign shard ids.
0.2 at the beginning
    -> A
1. master tell all n servers the new cluster size *
1.1. there are new shards to be created.
    if the shard is a copy of an existing shard, bootstrap from that shard first.
    if the shard is new, (with m <= shard_id < n), bootstrap from existing m shard As.
1.2 shard B follows all m shard As.
    -> A, -> B, B =>(i/n) A
2. master change the cluster size to n.
2.1 normal clients will read/write by new cluster size n.
3. master tell new cluster servers to commit the cluster size to disk
4. master tell shard B to stop, some servers may have a few shards to be removed.



Notes
-----
1. follow progress is stored with key as (server_address, shard_id), so
    any new following always starts from beginning.
    new shard needs to warm up and following from source shard for a while.


0 1 2
1 2 0

0 1
1 0
